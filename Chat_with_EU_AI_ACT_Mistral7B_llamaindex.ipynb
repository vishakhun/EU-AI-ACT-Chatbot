{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnPjFG44SAOzFB5ufZOAU0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishakhun/EU-AI-ACT-Chatbot/blob/main/Chat_with_EU_AI_ACT_Mistral7B_llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chat with EU AI ACT - Mistral7B and llamaindex\n",
        "This Colab notebook showcases the integration of advanced AI models for document processing and information retrieval, focusing on the EU AI Act dataset. Key tools and libraries used include:\n",
        "\n",
        "* Llama CPP and Langchain Embeddings: For advanced text processing and semantic analysis.\n",
        "* llama-index: To efficiently index and retrieve document data.\n",
        "* Transformers and Sentence-Transformers: For accessing pre-trained language models and embeddings.\n",
        "* Python Utilities: Such as logging and sys for effective logging and system operations.\n",
        "\n",
        "The notebook demonstrates the setup, configuration, and application of these technologies in creating a query engine capable of handling complex document-based queries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nh4mqwEM9hS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1: Environment Setup\n",
        "## 1.1 Installing Required Libraries"
      ],
      "metadata": {
        "id": "sY-mkbnPBI6s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlHJiitUz98W"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -q pypdf\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q transformers\n",
        "!pip install langchain\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install  llama-cpp-python --no-cache-dir\n",
        "!pip install -q llama-index\n",
        "!pip -q install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2: Basic Configurations\n",
        "## 2.1 Logging Configuration\n"
      ],
      "metadata": {
        "id": "R3J590R0BNk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "# Setting up the logging configuration\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext"
      ],
      "metadata": {
        "id": "NRtN-z2E03d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Data Loading and Processing\n",
        "## 3.1 Loading Documents\n"
      ],
      "metadata": {
        "id": "hKWlbfljBck3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/sample_data/EU_AI_ACT\").load_data()"
      ],
      "metadata": {
        "id": "bfRbHN6004i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4: Llama CPP Model Setup\n",
        "## 4.1 Initializing Llama CPP\n"
      ],
      "metadata": {
        "id": "ABwYFedZBpTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
        "# Detailed setup and initialization of the Llama CPP model\n",
        "    # Model URL and other configurations\n",
        "\n",
        "llm = LlamaCPP(\n",
        "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
        "    model_path=None,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    context_window=3900,\n",
        "    generate_kwargs={},\n",
        "    model_kwargs={\"n_gpu_layers\": -1},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "N6caQf_L1wLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5: Langchain Embeddings Integration\n",
        "## 5.1 Embedding Model Setup\n"
      ],
      "metadata": {
        "id": "pxvu4Ea6B4GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "from llama_index import ServiceContext\n",
        "# Setting up Langchain Embedding model\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\")\n",
        ")"
      ],
      "metadata": {
        "id": "iVVQUxM-8NiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6: Service Context and Indexing\n",
        "## 6.1 Creating Service Context\n"
      ],
      "metadata": {
        "id": "0-yfH_pFCBQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=256,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "RZWMSoQa2YD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Document Indexing\n"
      ],
      "metadata": {
        "id": "BpvtZcJKCHHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "Mpq_ym7h2cnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7: Query Engine and Responses"
      ],
      "metadata": {
        "id": "lE8COgJ6CMwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the query engine\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Which AI systems are considered high risk?\")"
      ],
      "metadata": {
        "id": "0C3RZFDx2g_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "5sFSNxHJ2kIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User query handling loop\n",
        "\n",
        "while True:\n",
        "  query=input()\n",
        "  response = query_engine.query(query)\n",
        "  print(response)"
      ],
      "metadata": {
        "id": "HSqdhhyc2mQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}